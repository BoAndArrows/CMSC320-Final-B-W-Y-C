{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e39a5b2",
   "metadata": {},
   "source": [
    "# Analysis of the White Fronted Goose (Anser albifrons) and Associated Subspecies\n",
    "\n",
    "Author: Waley Wang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99961641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import nessesary libraries and do nessesary non-df related prepwork\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import warnings\n",
    "\n",
    "import GooseUtils\n",
    "\n",
    "\n",
    "# Supress warning related to data types (will get on first import of the csv file)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed5f07",
   "metadata": {},
   "source": [
    "Import and trim relevant data. This notebook focuses only on data related to the Anser albifrons (species id: 1710) and its subspecies Anser albifrons elgasi (species id: 1719). This roughly corresponds to $222,322$ rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a370be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Retrieve the data from the csv file and filter out irrelevant species\n",
    "#\n",
    "\n",
    "# Retrieve group 1 data from the relevant CSV file\n",
    "goose_data_raw = pd.read_csv('Bird_Banding_Data/NABBP_2023_grp_01.csv')\n",
    "\n",
    "# Filter out all irrelevant species\n",
    "goose_data_raw = goose_data_raw[(goose_data_raw['SPECIES_ID'] == 1710) | (goose_data_raw['SPECIES_ID'] == 1719)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e7744b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 222322\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Get all relevant columns and display basic information about the data\n",
    "#\n",
    "\n",
    "# Retrieve all relevant columns\n",
    "goose_data = goose_data_raw[['BAND', \n",
    "                             'ORIGINAL_BAND', \n",
    "                             'OTHER_BANDS', \n",
    "                             'EVENT_DATE', \n",
    "                             'EVENT_DAY', \n",
    "                             'EVENT_MONTH', \n",
    "                             'EVENT_YEAR', \n",
    "                             'LAT_DD', \n",
    "                             'LON_DD', \n",
    "                             'COORD_PREC']]\n",
    "\n",
    "# Display number of non-null entries in each column\n",
    "print(f'Rows: {goose_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba1f4c",
   "metadata": {},
   "source": [
    "## 1: Data Cleaning\n",
    "\n",
    "Here all irrelevant entries in the data are filtered out and the data is formatted for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc39d4",
   "metadata": {},
   "source": [
    "### 1.1: Formatting Date\n",
    "\n",
    "A large number of the date cells (~ $3,295$) do not work with the `pd.to_datetime()` function. Since this is vital information for the analysis, the below cell aims specifically to clean the dates and remove any unessesary columns after. The following is the process by which dates are chosen.\n",
    "\n",
    "1. If the `'EVENT_DATE'` column already has a valid date that works with `pd.to_datetime()`, it will be the date used.\n",
    "2. Otherwise, if the `'EVENT_DAY'`, `'EVENT_MONTH'`, and `'EVENT_YEAR'` column all form a date that works with `pd.to_datetime()`, it will be the date used.\n",
    "3. If neither of the above work, `NaT` will be assigned and the row will be dropped.\n",
    "\n",
    "\n",
    "The `'EVENT_DAY'`, `'EVENT_MONTH'`, and `'EVENT_YEAR'` columns are all updated with the relevant information (will be used for grouping later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91df095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 219027\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Clean time-related columns as described above.\n",
    "#\n",
    "\n",
    "# Attempt to apply pd.to_datetime() to the EVENT_DATE column.\n",
    "goose_data['EVENT_DATE'] = pd.to_datetime(goose_data['EVENT_DATE'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Assemble date guesses from the EVENT_MONTH, EVENT_DAY, and EVENT_YEAR columns.\n",
    "dates_from_columns = pd.to_datetime(goose_data['EVENT_MONTH'].apply(str) + '/' + goose_data['EVENT_DAY'].apply(str) + '/' + goose_data['EVENT_YEAR'].apply(str), format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Fill in all NaT values that can be filled with the guesses from the previous line.\n",
    "goose_data['EVENT_DATE'] = goose_data['EVENT_DATE'].fillna(dates_from_columns)\n",
    "\n",
    "# Remove all rows where EVENT_DATE is still NaT after the above operations.\n",
    "goose_data = goose_data[goose_data['EVENT_DATE'].notna()]\n",
    "\n",
    "# Ammend the EVENT_DAY, EVENT_MONTH, and EVENT_YEAR columns based on the EVENT_DATE column.\n",
    "goose_data['EVENT_DAY'] = goose_data['EVENT_DATE'].apply(lambda x: x.day)\n",
    "goose_data['EVENT_MONTH'] = goose_data['EVENT_DATE'].apply(lambda x: x.month)\n",
    "goose_data['EVENT_YEAR'] = goose_data['EVENT_DATE'].apply(lambda x: x.year)\n",
    "\n",
    "print(f'Rows: {goose_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931cafd",
   "metadata": {},
   "source": [
    "### 1.2: Formatting Coordinates and Deriving a Coordinate Uncertainy\n",
    "\n",
    "Location data is also vital for analysis, so abit of cleaning will have to be done.\n",
    "\n",
    "First, rows fitting any of the following conditions will be excluded:\n",
    "1. Rows that do not have values for either `LAT_DD` or `LON_DD` because this issue cannot be rectified.\n",
    "2. Rows whose `COORD_PREC` values are `8`, `12`, `18`, `28`, `33`, `38`, `72`, or `NaN` because an uncertainty given either cannot be determined or is too big to be useful (Corresponds to $\\sim 1015$ entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "66769a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 218012\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Clean the coordinates columns as described above.\n",
    "#\n",
    "\n",
    "# Filter out all rows where LAT_DD or LON_DD are NaN. Cannot rectify rows with this issue.\n",
    "goose_data = goose_data[goose_data['LAT_DD'].notna() & goose_data['LON_DD'].notna()]\n",
    "\n",
    "# Filter out all rows with unusable or useless coordinate precision values as outlined above.\n",
    "goose_data = goose_data[~((goose_data['COORD_PREC'] == 8)  | \\\n",
    "                     (goose_data['COORD_PREC'] == 12) | \\\n",
    "                     (goose_data['COORD_PREC'] == 18) | \\\n",
    "                     (goose_data['COORD_PREC'] == 28) | \\\n",
    "                     (goose_data['COORD_PREC'] == 33) | \\\n",
    "                     (goose_data['COORD_PREC'] == 38) | \\\n",
    "                     (goose_data['COORD_PREC'] == 72) | \\\n",
    "                     (goose_data['COORD_PREC'].isna()))]\n",
    "\n",
    "print(f'Rows: {goose_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c7b3d",
   "metadata": {},
   "source": [
    "Additionally, a new column with lattitude and longitude uncertainties will be made whose values obey the following rules:\n",
    "1. If the `COORD_PREC` corresponds to an exact location (is `0`), then the uncertainty is $5*10^{6}$ to account for limits in the number of significant digits given by the data.\n",
    "2. If the `COORD_PREC` corresponds to a 1-minute block (is `1`), then the uncertainty is $\\frac{1}{120} \\approx 0.01$ degrees (rounded up) since the coordinates are in the centroid of the block.\n",
    "3. If the `COORD_PREC` corresponds to a 10-minute block (is `10`), then the uncertainty is $\\frac{1}{12} \\approx 0.1$ degrees (rounded up) since the coordinates are in the centroid of the block.\n",
    "4. If the `COORD_PREC` corresponds to a 1-degree block (is `60`), then the uncertainty is $0.5$ degrees since the coordinates are in the centroid of the block.\n",
    "5. If the `COORD_PREC` corresponds to a county (is `7`), then the uncertainty will be $0.25$ degrees by estimate (since the average county land area is 1090.69 degrees and a sqaure of that size is around $0.5$ degrees in lattitude and longitude)\n",
    "6. If the `COORD_PREC` corresponds to a town/area (is `11`), then the uncertainty will be $0.25$ degrees by estimate (since each town should be smaller than a county and thus have less uncertainty associated with it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1ea95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Perform the coordinate precision conversion as described above.\n",
    "#\n",
    "\n",
    "# Compute coording uncertainties\n",
    "goose_data['COORD_UNC'] = goose_data['COORD_PREC'].apply(lambda x : GooseUtils.get_coord_unc(x))\n",
    "\n",
    "# Drop the old column\n",
    "goose_data = goose_data.drop(labels=['COORD_PREC'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f785439",
   "metadata": {},
   "source": [
    "## 2: Perliminary Analysis\n",
    "\n",
    "### 2.1: Questions Related to Migration Patterns\n",
    "\n",
    "#### 2.1.1: Question 1: Does the White Fronted Goose Actually Migrate?\n",
    "\n",
    "For this, we will compare the median location of the bird over all datapoints taken within each month of the year. The median location here will be defined as the location whose lattitude is the median lattitude and whose longitude is the meadian longitude. We will also define locations being significantly different to mean that either the lattitude or longitude differs significantly.\n",
    "\n",
    "For this question, we will first handle the following hypotheses:\n",
    "\n",
    "Null Hypothesis ($H_0$): There is no significant differnce between the median positions of the White Fronted Goose between any two months.\n",
    "\n",
    "Alternate Hypothesis ($H_A$): This is a significant differnce between the median positions of the White Fronted Goose between any two months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b51e1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Group the data by EVENT_MONTH to prepare for further analysis\n",
    "#\n",
    "months_dict = goose_data.groupby('EVENT_MONTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c643ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kruskal-Wallis test statistic for lattitude: 136862.02851097204, p-value: 0.0\n",
      "Kruskal-Wallis test statistic for longitude: 54961.131582306334, p-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Perform the Kruskal-Wallis test on latitude and longitude data\n",
    "#\n",
    "\n",
    "# Prepare lists containing lattidue and longitude data from each month\n",
    "lat_smaple = [group['LAT_DD'].values for _, group in months_dict]\n",
    "lon_sample = [group['LON_DD'].values for _, group in months_dict]\n",
    "\n",
    "# Perform the Kruskal-Wallis test on latitude and longitude data\n",
    "lat_stat, lat_p_value = stats.kruskal(*lat_smaple)\n",
    "lon_stat, lon_p_value = stats.kruskal(*lon_sample)\n",
    "\n",
    "# Print out the results\n",
    "print(f\"Kruskal-Wallis test statistic for lattitude: {lat_stat}, p-value: {lat_p_value}\")\n",
    "print(f\"Kruskal-Wallis test statistic for longitude: {lon_stat}, p-value: {lon_p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1389538",
   "metadata": {},
   "source": [
    "As seen above, the Kruskal-Wallis statistic is around $137,000$ for lattitude and around $55,000$ for longitude. This corresponds with extremely small $p$ values for both lattitude and longitude (both too small for Python to properly store). This means that the median location of the White Fronted Goose differs significantly between at least two months since the $p$ value is clearly smaller than $0.05$, the threshold required to reject the null hypothesis $H_0$. Therefore we can only conclude that $H_A$ is true.\n",
    "\n",
    "Since $H_0$ was rejected, a post-hoc analysis is at hand. This will be a series of $\\frac{12(12+1)}{2} = 52$ sets of null and alternate hypotheses (denoted as $H_{(0, i, j)}$ and $H_{(A, i, j)}$ for $i, j \\in \\{1, 2, ..., 12 \\} | i \\neq j$) defined as follows:\n",
    "\n",
    "Null Hypothesis ($H_{(0, i, j)}$): The median position in month $i$ does not differ significantly from $j$.\n",
    "\n",
    "Alternate Hypothesis ($H_{(A, i, j)}$): The median position in month $i$ differs significantly from $j$.\n",
    "\n",
    "This will be tested with Mann Witney Tests. The condition for rejecting the null is set to be $p < 0.05$ for either lattitude or longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e3a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1     2     3     4     5     6     7     8     9      10     11    12\n",
       "1   None  True  True  True  True  True  True  True  True   True   True  True\n",
       "2   None  None  True  True  True  True  True  True  True   True  False  True\n",
       "3   None  None  None  True  True  True  True  True  True   True   True  True\n",
       "4   None  None  None  None  True  True  True  True  True   True   True  True\n",
       "5   None  None  None  None  None  True  True  True  True   True   True  True\n",
       "6   None  None  None  None  None  None  True  True  True   True   True  True\n",
       "7   None  None  None  None  None  None  None  True  True   True   True  True\n",
       "8   None  None  None  None  None  None  None  None  True   True   True  True\n",
       "9   None  None  None  None  None  None  None  None  None  False   True  True\n",
       "10  None  None  None  None  None  None  None  None  None   None   True  True\n",
       "11  None  None  None  None  None  None  None  None  None   None   None  True\n",
       "12  None  None  None  None  None  None  None  None  None   None   None  None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize a 12x12 matrix to store the results of the Mann-Whitney U test\n",
    "mann_whitney_results = np.full((12, 12), None, dtype=object)\n",
    "\n",
    "# Perform the Mann-Whitney U test for each relevant pair of months\n",
    "for i in range(0, 12):\n",
    "    for j in range(i + 1, 12):\n",
    "        lat_stat, lat_p_value = stats.mannwhitneyu(months_dict.get_group(i + 1)['LAT_DD'], months_dict.get_group(j + 1)['LAT_DD'], alternative='two-sided')\n",
    "        lon_stat, lon_p_value = stats.mannwhitneyu(months_dict.get_group(i + 1)['LON_DD'], months_dict.get_group(j + 1)['LON_DD'], alternative='two-sided')\n",
    "        mann_whitney_results[i][j] = (lat_stat, lat_p_value, lon_stat, lon_p_value)\n",
    "\n",
    "# Store the results into a DataFrame for better visualization\n",
    "mann_whitney_df = pd.DataFrame(mann_whitney_results, index=range(1, 13), columns=range(1, 13))\n",
    "\n",
    "# Extract the relevant statistics and p-values from the DataFrame\n",
    "mw_lat_stat_df = mann_whitney_df.map(lambda x: x[0] if x is not None else None)\n",
    "mw_lat_p_df = mann_whitney_df.map(lambda x: x[1] if x is not None else None)\n",
    "mw_lon_stat_df = mann_whitney_df.map(lambda x: x[2] if x is not None else None)\n",
    "mw_lon_p_df = mann_whitney_df.map(lambda x: x[3] if x is not None else None)\n",
    "\n",
    "# Table whose values correspond to whetehr the null hypothesis can be rejected for each pair of months\n",
    "mw_h0_reject_df = (mann_whitney_df.map(lambda x: (x[1], x[3]) if x is not None else None)).map(lambda x: x[0] < 0.05 and x[1] < 0.05 if x is not None else None)\n",
    "\n",
    "# Display the results\n",
    "#     True  -> Reject Null\n",
    "#     False -> Fail to Reject\n",
    "display(mw_h0_reject_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6901371",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
